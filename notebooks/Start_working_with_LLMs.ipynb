{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "593aa10d-0c7d-464c-b559-5d23a2052e0d",
   "metadata": {},
   "source": [
    "## Abstraction layer to work with any LLM API\n",
    "\n",
    "This Python code defines a class called `MyLLM` that provides an interface to interact with three different language model APIs: OpenAI, Cohere, and Anthropic. Here's a breakdown of the code:\n",
    "\n",
    "### Import Statements\n",
    "```python\n",
    "import openai\n",
    "import cohere\n",
    "import anthropic\n",
    "```\n",
    "These import the necessary libraries for each respective API.\n",
    "\n",
    "### Class Definition\n",
    "- `__init__` initializes the class with the type of API (`api_type`), an API key (`api_key`), and the model to use.\n",
    "- Depending on the `api_type` provided, it creates a client for the corresponding API and assigns it to `self.client`.\n",
    "\n",
    "### Generate Response Method\n",
    "- `generate_response` generates a response based on the messages provided.\n",
    "- It constructs the appropriate prompt for the selected API and then makes an API call to generate a response.\n",
    "- The response format and method to access the generated text differ for each API.\n",
    "\n",
    "### Helper Methods for Constructing Prompts\n",
    "- These methods construct the prompt for the respective APIs from the provided messages.\n",
    "- For Cohere and Anthropic, it concatenates messages into a single string with appropriate roles (`System`, `User`, `Assistant`).\n",
    "- The prompt ends with `Assistant:` to indicate that the next text generated should be from the assistant.\n",
    "\n",
    "### Summary\n",
    "This class provides a unified interface to generate responses using different language model APIs. The key steps include:\n",
    "1. Initializing the class with the appropriate API client.\n",
    "2. Constructing prompts suitable for the selected API.\n",
    "3. Making API calls to generate responses and extracting the relevant content from the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72155d74-95cf-4ffb-b7bc-2ff4b15302a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import cohere\n",
    "import anthropic\n",
    "\n",
    "class MyLLM:\n",
    "    def __init__(self, api_type, api_key, model):\n",
    "        self.api_type = api_type\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        \n",
    "        if api_type == 'openai':\n",
    "            self.client = openai.OpenAI()\n",
    "            self.client.api_key = api_key\n",
    "        elif api_type == 'cohere':\n",
    "            self.client = cohere.Client(api_key=api_key)\n",
    "        elif api_type == 'anthropic':\n",
    "            self.client = anthropic.Client(api_key=api_key)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported API type. Choose 'openai', 'cohere', or 'anthropic'.\")\n",
    "\n",
    "    def generate_response(self, messages, temperature=0.0, max_tokens=1000):\n",
    "        if self.api_type == 'openai':\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        elif self.api_type == 'cohere':\n",
    "            response = self.client.chat(\n",
    "                model=self.model,\n",
    "                message=messages,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                stop_sequences=[\"USER:\", \"SYSTEM:\", \"CHATBOT:\"]\n",
    "            )\n",
    "            return response.text\n",
    "        elif self.api_type == 'anthropic':\n",
    "            response = self.client.completions.create(\n",
    "                model=self.model,\n",
    "                prompt=messages,\n",
    "                max_tokens_to_sample=max_tokens,\n",
    "                temperature=temperature,\n",
    "                stop_sequences=[\"User:\", \"System:\", \"Assistant:\"]\n",
    "            )\n",
    "            return response[\"completion\"].strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a86c61a",
   "metadata": {},
   "source": [
    "## Main models available for each API\n",
    "\n",
    "## OpenAI Models\n",
    "\n",
    "| Model Name          | Code                       | Description                            |\n",
    "|---------------------|----------------------------|----------------------------------------|\n",
    "| GPT-4o              | gpt-4o                     | Fastest and most affordable.           |\n",
    "| GPT-4 Turbo         | gpt-4-turbo                | Similar to GPT-4o but slower.          |\n",
    "| GPT-3.5 Turbo       | gpt-3.5-turbo              | Fast and inexpensive.                  |\n",
    "| DALL·E 3            | dall-e-3                   | Generates and edits images.            |\n",
    "| DALL·E 2            | dall-e-2                   | Generates and edits images.            |\n",
    "| TTS                 | tts-1                      | Text to speech.                        |\n",
    "| TTS HD              | tts-1-hd                   | High-definition text to speech.        |\n",
    "| Whisper             | whisper-1                  | Audio to text.                         |\n",
    "| Embeddings Large    | text-embedding-3-large     | Text to embeddings.                    |\n",
    "| Embeddings Small    | text-embedding-3-small     | Text to embeddings.                    |\n",
    "| Embeddings Ada      | text-embedding-ada-002     | Text to embeddings.                    |\n",
    "| Moderation Latest   | text-moderation-latest     | Detects sensitive text.                |\n",
    "| Moderation Stable   | text-moderation-stable     | Detects sensitive text.                |\n",
    "| Moderation 007      | text-moderation-007        | Detects sensitive text.                |\n",
    "| GPT Base Babbage    | babbage-002                | Base language model.                   |\n",
    "| GPT Base Davinci    | davinci-002                | Base language model.                   |\n",
    "\n",
    "## Cohere API Models\n",
    "\n",
    "| Model Name                  | Code                        | Description                            |\n",
    "|-----------------------------|-----------------------------|----------------------------------------|\n",
    "| Command R+                  | command-r-plus              | High-quality instruction model.        |\n",
    "| Command R                   | command-r                   | High-quality instruction model.        |\n",
    "| Command                     | command                     | High-quality instruction model.        |\n",
    "| Command Light               | command-light               | Fast, smaller instruction model.       |\n",
    "| Embed English v3.0          | embed-english-v3.0          | English embeddings.                    |\n",
    "| Embed English Light v3.0    | embed-english-light-v3.0    | English embeddings.                    |\n",
    "| Embed Multilingual v3.0     | embed-multilingual-v3.0     | Multilingual embeddings.               |\n",
    "| Embed Multilingual Light v3.0 | embed-multilingual-light-v3.0 | Multilingual embeddings.               |\n",
    "| Embed English v2.0          | embed-english-v2.0          | English embeddings.                    |\n",
    "| Embed English Light v2.0    | embed-english-light-v2.0    | English embeddings.                    |\n",
    "| Embed Multilingual v2.0     | embed-multilingual-v2.0     | Multilingual embeddings.               |\n",
    "\n",
    "## Anthropic API Models\n",
    "\n",
    "| Model Name            | Code                       | Description                            |\n",
    "|-----------------------|----------------------------|----------------------------------------|\n",
    "| Claude 3.5 Sonnet     | claude-3-5-sonnet-20240620 | Latest Claude 3.5 model.               |\n",
    "| Claude 3 Opus         | claude-3-opus-20240229     | Claude 3 optimized for platforms.      |\n",
    "| Claude 3 Sonnet       | claude-3-sonnet-20240229   | Claude 3 for API integrations.         |\n",
    "| Claude 3 Haiku        | claude-3-haiku-20240307    | Claude 3 for cloud platforms.          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7dc8a2",
   "metadata": {},
   "source": [
    "### Using the `LLM_Messages` Class for Different API Types\n",
    "\n",
    "This part demonstrates how to use the `LLM_Messages` class to create and manage messages for different Large Language Model (LLM) APIs, namely OpenAI, Cohere, and Anthropic. \n",
    "\n",
    "We will:\n",
    "1. Define the `LLM_Messages` class and the `message_models` dictionary.\n",
    "2. Create instances of `LLM_Messages` for each API type.\n",
    "3. Add system, human, and assistant messages.\n",
    "4. Retrieve messages in different formats.\n",
    "5. Export messages to JSON format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ef55b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define message models for different APIs\n",
    "message_models = {\n",
    "    'openai': {\n",
    "        'roles': {\n",
    "            'in': 'user',\n",
    "            'out': 'assistant',\n",
    "            'system': 'system'\n",
    "        },\n",
    "        'output_field': 'content'\n",
    "    },\n",
    "    'cohere': {\n",
    "        'roles': {\n",
    "            'in': 'USER',\n",
    "            'out': 'CHATBOT',\n",
    "            'system': 'SYSTEM'\n",
    "        },\n",
    "        'output_field': 'text'\n",
    "    },\n",
    "    'anthropic': {\n",
    "        'roles': {\n",
    "            'in': 'user',\n",
    "            'out': 'assistant',\n",
    "            'system': 'system'\n",
    "        },\n",
    "        'output_field': 'content'\n",
    "    },\n",
    "}\n",
    "\n",
    "# Define the LLM_Messages class\n",
    "class LLM_Messages:\n",
    "    def __init__(self, api_type):\n",
    "        if api_type in [ 'openai', 'cohere', 'anthropic' ]:\n",
    "            self.api_type = api_type\n",
    "            self.message_model=message_models[api_type]\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported API type. Choose 'openai', 'cohere', or 'anthropic'.\")\n",
    "\n",
    "        self.messages = []\n",
    "\n",
    "    def add_system_message(self, message):\n",
    "        self.messages.append({\"role\": self.message_model['roles']['system'], self.message_model['output_field']: message})\n",
    "\n",
    "    def add_human_message(self, message):\n",
    "        self.messages.append({\"role\": self.message_model['roles']['in'], self.message_model['output_field']: message})\n",
    "\n",
    "    def add_assistant_message(self, message):\n",
    "        self.messages.append({\"role\": self.message_model['roles']['out'], self.message_model['output_field']: message})\n",
    "        \n",
    "    def add_prompt(self, message):\n",
    "        self.add_human_message(message)\n",
    "\n",
    "    def add_response(self, message):\n",
    "        self.add_assistant_message(message)\n",
    "\n",
    "    def get_messages(self):\n",
    "        return self.messages\n",
    "    \n",
    "    def get_api_messages(self):\n",
    "        if self.api_type == 'openai':\n",
    "            return self.messages\n",
    "        if self.api_type == 'cohere':\n",
    "            prompt = \"\"\n",
    "            for message in self.messages:\n",
    "                prompt += f\"{message['role']}: {message['text']}\\n\"\n",
    "            return prompt + self.message_model['roles']['out'] + \": \"\n",
    "        if self.api_type == 'anthropic':\n",
    "            prompt = \"\"\n",
    "            for message in self.messages:\n",
    "                prompt += f\"{message['role']}: {message['text']}\\n\"\n",
    "            return prompt + self.message_model['roles']['out'] + \": \"\n",
    "            \n",
    "    def to_json(self):\n",
    "        return json.dumps(self.get_messages(), indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f980249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# Create instances for each API type\n",
    "openai_messages = LLM_Messages(api_type='openai')\n",
    "cohere_messages = LLM_Messages(api_type='cohere')\n",
    "anthropic_messages = LLM_Messages(api_type='anthropic')\n",
    "\n",
    "# Add messages for OpenAI\n",
    "openai_messages.add_system_message(\"You are an assistant.\")\n",
    "openai_messages.add_human_message(\"What is the weather today?\")\n",
    "openai_messages.add_assistant_message(\"The weather today is sunny with a high of 25°C.\")\n",
    "\n",
    "# Add messages for Cohere\n",
    "cohere_messages.add_system_message(\"You are a helpful chatbot.\")\n",
    "cohere_messages.add_human_message(\"Tell me a joke.\")\n",
    "cohere_messages.add_assistant_message(\"Why don't scientists trust atoms? Because they make up everything!\")\n",
    "\n",
    "# Add messages for Anthropic\n",
    "anthropic_messages.add_system_message(\"You are an AI assistant.\")\n",
    "anthropic_messages.add_human_message(\"How do I bake a cake?\")\n",
    "anthropic_messages.add_assistant_message(\"To bake a cake, you need to follow a recipe with steps such as mixing ingredients, preheating the oven, and baking for a specified time.\")\n",
    "\n",
    "# Retrieve messages\n",
    "print(\"OpenAI Messages:\", openai_messages.get_messages())\n",
    "print(\"Cohere Messages:\", cohere_messages.get_messages())\n",
    "print(\"Anthropic Messages:\", anthropic_messages.get_messages())\n",
    "\n",
    "# Retrieve API-specific message format\n",
    "print(\"OpenAI API Messages:\", openai_messages.get_api_messages())\n",
    "print(\"Cohere API Messages:\", cohere_messages.get_api_messages())\n",
    "print(\"Anthropic API Messages:\", anthropic_messages.get_api_messages())\n",
    "\n",
    "# Export to JSON\n",
    "print(\"OpenAI Messages JSON:\", openai_messages.to_json())\n",
    "print(\"Cohere Messages JSON:\", cohere_messages.to_json())\n",
    "print(\"Anthropic Messages JSON:\", anthropic_messages.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e47474",
   "metadata": {},
   "source": [
    "# Worker Class\n",
    "\n",
    "The `Worker` class is designed to process tasks based on the specified agent profiles. It includes methods for adding system messages, human messages, assistant messages, prompts, responses, and generating responses. It also manages task processing, clearing messages, clearing results, and resetting the worker.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a929ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "#from myllm import MyLLM\n",
    "#from llm_messages import LLM_Messages\n",
    "\n",
    "class Worker:\n",
    "    def __init__(self, name, api_type, api_key, model, profile=None, has_memory=False):\n",
    "        self.name = name\n",
    "        self.api_type = api_type\n",
    "        self.llm_instance = MyLLM(api_type=api_type, api_key=api_key, model=model)\n",
    "        self.llm_messages = LLM_Messages(api_type=api_type)\n",
    "        self.has_memory = has_memory\n",
    "        self.system_message = \"\"\n",
    "        self.results = []\n",
    "        \n",
    "        if profile is not None:\n",
    "            self.profile = profile\n",
    "            self.add_system_message(profile['system_message'])\n",
    "\n",
    "    def add_system_message(self, message):\n",
    "        self.system_message = message\n",
    "        self.llm_messages.add_system_message(message)\n",
    "\n",
    "    def add_human_message(self, message):\n",
    "        self.llm_messages.add_human_message(message)\n",
    "\n",
    "    def add_assistant_message(self, message):\n",
    "        self.llm_messages.add_assistant_message(message)\n",
    "\n",
    "    def add_prompt(self, message):\n",
    "        self.llm_messages.add_prompt(message)\n",
    "        \n",
    "    def add_prompts(self, messages):\n",
    "        for message in messages:\n",
    "            self.llm_messages.add_prompt(message)\n",
    "\n",
    "    def add_response(self, message):\n",
    "        self.llm_messages.add_response(message)\n",
    "\n",
    "    def generate_response(self, temperature=0.0, max_tokens=1000, is_message=False):\n",
    "        messages = self.llm_messages.get_api_messages()\n",
    "        response = self.llm_instance.generate_response(messages, temperature=temperature, max_tokens=max_tokens)\n",
    "        if not is_message:\n",
    "            self.add_response(response)\n",
    "        return response\n",
    "\n",
    "    def process_tasks(self, prompt=None, temperature=0.0, max_tokens=1000, is_message=False):\n",
    "        if self.profile is None:\n",
    "            raise ValueError(\"Profile is not set. Cannot process tasks.\")\n",
    "        work_result = []\n",
    "        if prompt is not None:\n",
    "            self.add_prompt(prompt)\n",
    "        for task in self.profile['tasks']:\n",
    "            self.add_prompt(task['task'])\n",
    "            result = self.generate_response(temperature=temperature, max_tokens=max_tokens, is_message=is_message)\n",
    "            work_result.append(result)\n",
    "        if self.has_memory:\n",
    "            self.results.extend(work_result)\n",
    "        else:\n",
    "            self.reset()\n",
    "        return work_result\n",
    "    \n",
    "    def clear_messages(self):\n",
    "        del self.llm_messages\n",
    "        gc.collect()\n",
    "        self.llm_messages = LLM_Messages(api_type=self.api_type)\n",
    "        self.llm_messages.add_system_message(self.system_message)\n",
    "\n",
    "    def clear_results(self):\n",
    "        del self.results\n",
    "        gc.collect()\n",
    "        self.results = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.clear_messages()\n",
    "        self.clear_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67614cdc",
   "metadata": {},
   "source": [
    "# A proposal for a resolution of a problem described in a Github issue\n",
    "\n",
    "The following diagram illustrates a structured workflow designed to manage and resolve a GitHub issue through a system of specialized agents, each assigned specific roles and tasks. This process begins with the identification of an issue and progresses through a series of steps involving various agents such as the Overviewer, Decider, Code Reasoner, DB Analyst, Code Generator, and Assistant. Each agent contributes its expertise, ensuring that the issue is comprehensively analyzed, tasks are efficiently assigned, solutions are accurately developed, and all relevant information is gathered for final resolution.\n",
    "\n",
    "```mermaid\n",
    "stateDiagram-v2\n",
    "    [Issue] --> Overviewer\n",
    "    Overviewer --> Decider\n",
    "    Decider --> Code_Reasoner\n",
    "    Decider --> DB_Analyst\n",
    "    Decider --> Assistant\n",
    "    Code_Reasoner --> Code_Generator\n",
    "    Code_Generator --> [*]\n",
    "    DB_Analyst --> [*]\n",
    "    Assistant --> [*]\n",
    "```\n",
    "\n",
    "The diagram describes the workflow of handling a GitHub issue using a system of specialized agents, each with a specific role. Here's a step-by-step explanation of the diagram:\n",
    "\n",
    "1. **Issue**:\n",
    "    - The process starts with an issue being identified or received.\n",
    "\n",
    "2. **Overviewer**:\n",
    "    - The issue is first sent to the Overviewer.\n",
    "    - The Overviewer is responsible for generating an overview of the issue by identifying the problem, setting objectives, providing specific details, and including relevant background information.\n",
    "\n",
    "3. **Decider**:\n",
    "    - After the Overviewer generates an overview, the information is passed to the Decider.\n",
    "    - The Decider's role is to analyze the tasks required and decide which specialized agent should handle each task.\n",
    "\n",
    "4. **Code Reasoner, DB Analyst, and Assistant**:\n",
    "    - The Decider can choose to send tasks to the Code Reasoner, DB Analyst, or Assistant, depending on the nature of the tasks.\n",
    "    - **Code Reasoner**: Responsible for documenting requirements, designing solutions, and analyzing the environment needed to run the solution.\n",
    "    - **DB Analyst**: Responsible for database-related analyses and producing database instructions.\n",
    "    - **Assistant**: Acts as a versatile helper, taking on tasks that do not specifically fall under the expertise of the Code Reasoner or DB Analyst.\n",
    "\n",
    "5. **Code Generator**:\n",
    "    - If the Decider assigns tasks to the Code Reasoner, the Code Reasoner may further pass specific sub-tasks to the Code Generator.\n",
    "    - The Code Generator is responsible for writing the necessary code to implement solutions and developing tests for different cases, including edge cases.\n",
    "\n",
    "6. **Final State**:\n",
    "    - The workflow reaches its final state after the Code Generator, DB Analyst, or Assistant completes their respective tasks.\n",
    "    - This end point signifies that all necessary information has been gathered and processed, leading to the final resolution or closure of the issue.\n",
    "\n",
    "\n",
    "The workflow ensures that each aspect of the issue is handled by the appropriate specialized agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e636bf3f",
   "metadata": {},
   "source": [
    "## Profiles Overview\n",
    "\n",
    "### Overviewer Profile\n",
    "\n",
    "The `Overviewer` profile is designed to generate an overview of a GitHub issue. It includes tasks for identifying and articulating the problem, setting objectives, setting sub-problems, providing specific details, and including relevant background information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf99f16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Overviewer_profile = {\n",
    "    \"name\": \"Overviewer\",\n",
    "    \"description\": \"Overviewer is an agent that generates an overview of a GitHub issue.\",\n",
    "    \"system_message\": \"You are a system analyst and a problem solver.\",\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"task\": \"\"\"\n",
    "            **Instructions for Articulating the Problem:**\n",
    "\n",
    "                1. **Identify the Problem**: Clearly identify and articulate the problem you are addressing.\n",
    "\n",
    "                2. **Clearly set the objective**: Clearly set the objective and the means to solve the problem.\n",
    "\n",
    "                3. **Provide Specific Details and Context**: Offer specific details and context to ensure a comprehensive understanding of the problem.\n",
    "\n",
    "                4. **Include Relevant Background Information**: Include any relevant background information that may help in understanding the problem.\n",
    "\n",
    "                5. **Default Development Context**: If no specific development context is provided, assume the following:\n",
    "                    - **Programming Language**: Python\n",
    "                    - **Virtual Environment**: venv\n",
    "                    - **Testing Frameworks**: pytest and vcrpy\n",
    "                \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"task\": \"\"\"\n",
    "            **Instructions for Dividing the Problem into Sub-Problems:**\n",
    "            1. **Divide the Problem**: Break down the main problem into smaller, manageable parts called sub-problems.\n",
    "            2. **Identify Inputs and Outputs**: For each sub-problem, clearly identify the specific inputs required and the expected outputs.\n",
    "            3. **Detailed Breakdown**: Provide a thorough and detailed breakdown of each sub-problem.\n",
    "            4. **Sub-Problem Description**: Begin the description of each sub-problem with '#Sub-problem'.\n",
    "            \n",
    "            The output format must be JSON.\n",
    "            \n",
    "            ---\n",
    "\n",
    "            **Example:**\n",
    "\n",
    "            {\n",
    "            \"sub_problems\": [\n",
    "                {\n",
    "                \"id\": 1,\n",
    "                \"title\": \"Sub-problem 1 Title\",\n",
    "                \"inputs\": \"List the specific inputs needed for this sub-problem.\",\n",
    "                \"outputs\": \"List the expected outputs from this sub-problem.\",\n",
    "                \"description\": \"Provide a detailed explanation of the tasks, steps, or calculations involved.\"\n",
    "                },\n",
    "                {\n",
    "                \"id\": 2,\n",
    "                \"title\": \"Sub-problem 2 Title\",\n",
    "                \"inputs\": \"List the specific inputs needed for this sub-problem.\",\n",
    "                \"outputs\": \"List the expected outputs from this sub-problem.\",\n",
    "                \"description\": \"Provide a detailed explanation of the tasks, steps, or calculations involved.\"\n",
    "                },\n",
    "                {\n",
    "                \"id\": N,\n",
    "                \"title\": \"Sub-problem N Title\",\n",
    "                \"inputs\": \"List the specific inputs needed for this sub-problem.\",\n",
    "                \"outputs\": \"List the expected outputs from this sub-problem.\",\n",
    "                \"description\": \"Provide a detailed explanation of the tasks, steps, or calculations involved.\"\n",
    "                }\n",
    "            ]\n",
    "            }\n",
    "\n",
    "            ---\n",
    "\n",
    "            Note that these sub-problems will be resolved by a machine, not a human.\n",
    "            \"\"\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa80a67",
   "metadata": {},
   "source": [
    "### Decider Profile\n",
    "\n",
    "The `Decider` profile is responsible for deciding which worker to consult based on the task at hand. It includes options for different agents and outputs the decision in JSON format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d869245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Decider_profile = {\n",
    "    \"name\": \"Decider\",\n",
    "    \"description\": \"Decider is an agent that decides which worker to consult.\",\n",
    "    \"system_message\": \"You are a problem solver and a wise decision maker.\",\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"task\": \"\"\"\n",
    "                According to the task at hand, take a wise decision on which worker should perform it.\n",
    "                Options:\n",
    "                    1. Code Reasoner (Code Reasoner is an agent that documents the requirements, designs the solution and analyses the environment to run the solution)\n",
    "                    2. Database Engineer (Database Engineer is an agent performs database analyses and produces database instructions)\n",
    "                    3. Assistant (Choose Assistant as a versatile helper only when no other worker is suited for the task.)\n",
    "                \n",
    "                The output format must be JSON.\n",
    "                \n",
    "                ---\n",
    "                \n",
    "                **Example:**\n",
    "\n",
    "                {\n",
    "                \"decision\": \"Code Reasoner\"\n",
    "                }\n",
    "            \"\"\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d54975",
   "metadata": {},
   "source": [
    "### Code Reasoner Profile\n",
    "\n",
    "The `Code Reasoner` profile documents the requirements, designs the solution, and analyzes the environment needed to run the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b581729",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code_Reasoner_profile = {\n",
    "    \"name\": \"Code Reasoner\",\n",
    "    \"description\": \"Code Reasoner is an agent that documents the requirements, designs the solution and analyses the environment to run the solution.\",\n",
    "    \"system_message\": \"You are a system analyst.\",\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"task\": \"\"\"\n",
    "                List all requirements and constraints for solving the problem.\n",
    "                Include functional requirements, performance criteria, and any\n",
    "                limitations or assumptions that need to be considered.\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"task\": \"\"\"\n",
    "                Create a high-level design for the solution. Outline the logic\n",
    "                and steps needed to solve the problem.\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"task\": \"\"\"\n",
    "                Ensure all necessary software and libraries are installed. \n",
    "                Provide a list of required tools and instructions for setting up the development environment. \n",
    "                Verify that the environment is correctly configured for the task.\n",
    "            \"\"\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af33818d",
   "metadata": {},
   "source": [
    "### Code Generator Profile\n",
    "\n",
    "The `Code Generator` profile develops programming code and writes tests to cover different cases, including edge cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ed15a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code_Generator_profile = {\n",
    "    \"name\": \"Code Generator\",\n",
    "    \"description\": \"Code Generator is an agent that develops programming code.\",\n",
    "    \"system_message\": \"You are a software developer.\",\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"task\": \"\"\"\n",
    "                Your response must contain only the necessary code,\n",
    "                with minimal and commented textual descriptions.\n",
    "                Write the code to implement a solution for the sub-problem.\n",
    "\n",
    "                The output format must be JSON.\n",
    "                \n",
    "                ---\n",
    "\n",
    "                **Example:**\n",
    "\n",
    "                {\n",
    "                \"output_files\": [\n",
    "                    {\n",
    "                    \"name\": \"some_output_file.py\",\n",
    "                    \"content\": \"...\"\n",
    "                    },\n",
    "                    ...\n",
    "                    ]\n",
    "                }\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"task\": \"\"\"\n",
    "                Write the tests to cover different cases, including edge cases.\n",
    "\n",
    "                The output format must be JSON.\n",
    "                \n",
    "                ---\n",
    "\n",
    "                **Example:**\n",
    "\n",
    "                {\n",
    "                \"output_files\": [\n",
    "                    {\n",
    "                    \"name\": \"some_output_file.py\",\n",
    "                    \"content\": \"...\"\n",
    "                    },\n",
    "                    ...\n",
    "                    ]\n",
    "                }\n",
    "            \"\"\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f224c6",
   "metadata": {},
   "source": [
    "### DB Engineer Profile\n",
    "\n",
    "The `DB Engineer` profile performs database analyses and produces database instructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e46ac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_Engineer_profile = {\n",
    "    \"name\": \"DB_Engineer\",\n",
    "    \"description\": \"DB_Engineer is an agent performs database analyses and produces database instructions.\",\n",
    "    \"system_message\": \"\"\"\n",
    "                    You are a database engineer. Your role typically includes tasks such as:\n",
    "                    1. **Database Design**: Creating the structure of the database, including tables, relationships, and indexes.\n",
    "                    2. **Database Construction**: Building the database according to the design, often using SQL (Structured Query Language) or other database management tools.\n",
    "                    3. **Writing Instructions**: Developing queries, stored procedures, triggers, and other database components to ensure the database functions efficiently and meets the needs of the application or user.\n",
    "    \"\"\",\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"task\": \"\"\"\n",
    "                Given the context, develop code for the task.\n",
    "            \"\"\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3616546",
   "metadata": {},
   "source": [
    "### Assistant Profile\n",
    "\n",
    "The `Assistant` profile is a general-purpose helper that attempts to solve tasks to the best of its ability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49301241",
   "metadata": {},
   "outputs": [],
   "source": [
    "Assistant_profile = {\n",
    "    \"name\": \"Assistant\",\n",
    "    \"description\": \"Assistant is a general purpose helper.\",\n",
    "    \"system_message\": \"You are a helpful assistant.\",\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"task\": \"\"\"\n",
    "                According to the task at hand, make your best effort to help solve the task.\n",
    "                \n",
    "                The output format must be JSON.\n",
    "                \n",
    "                ---\n",
    "                \n",
    "                **Example:**\n",
    "\n",
    "                {\n",
    "                \"resolution\": \"...\"\n",
    "                }\n",
    "            \"\"\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832be927",
   "metadata": {},
   "source": [
    "# Implementation example of the concepts presented above\n",
    "\n",
    "In this section, we will delve into a practical implementation example to illustrate the concepts discussed previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77ce5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def get_github_issue(owner, repo, issue_number, token):\n",
    "    url = f\"https://api.github.com/repos/{owner}/{repo}/issues/{issue_number}\"\n",
    "    \n",
    "    headers = {}\n",
    "    if token:\n",
    "        headers['Authorization'] = f'Bearer {token}'\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    elif response.status_code == 403:\n",
    "        raise Exception(\"API rate limit exceeded. Please try again later.\")\n",
    "    elif response.status_code == 404:\n",
    "        raise Exception(f\"Issue {issue_number} not found in {owner}/{repo}.\")\n",
    "    else:\n",
    "        raise Exception(f\"Failed to fetch issue {issue_number} from {owner}/{repo}. HTTP Status Code: {response.status_code}\")\n",
    "\n",
    "def get_data_from_url(url):\n",
    "    splits = url.split('/')\n",
    "    if len(splits) >= 5:\n",
    "        return {\n",
    "            'owner': splits[-4],\n",
    "            'repo': splits[-3],\n",
    "            'issue': splits[-1]\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(\"Invalid URL format. Expected format: https://github.com/<owner>/<repo>/issues/<issue_number>\")\n",
    "\n",
    "def cleanJSON(s):\n",
    "    s = s[next(idx for idx, c in enumerate(s) if c in \"{[\"):]\n",
    "    try:\n",
    "        return json.loads(str(s))\n",
    "    except json.JSONDecodeError as e:\n",
    "        return json.loads(s[:e.pos])\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    if not args:\n",
    "        print(\"Usage: test.py <github_issue_url>\")\n",
    "        sys.exit(1)\n",
    "    try:\n",
    "        load_dotenv()\n",
    "\n",
    "        openai_api_key=os.environ['OPENAI_API_KEY']\n",
    "        cohere_api_key=os.environ['COHERE_API_KEY']\n",
    "        anthropic_api_key=os.environ['ANTHROPIC_API_KEY']\n",
    "        github_api_key=os.environ['GITHUB_ACCESS_TOKEN']\n",
    "        \n",
    "        url = args[0]\n",
    "        issue_data = get_data_from_url(url)\n",
    "        issue = get_github_issue(issue_data['owner'], issue_data['repo'], issue_data['issue'], github_api_key)\n",
    "        issue_body = issue['body']\n",
    "\n",
    "        api_type_in_use = 'cohere'\n",
    "        model_in_use = 'command-r-plus'\n",
    "        llm_api_key=cohere_api_key\n",
    "\n",
    "        worker_overviewer = Worker(\"Overviewer\", api_type=api_type_in_use, api_key=llm_api_key, model=model_in_use, profile=Overviewer_profile)\n",
    "        worker_decider = Worker(\"Decider\", api_type=api_type_in_use, api_key=llm_api_key, model=model_in_use, profile=Decider_profile)\n",
    "        worker_db_engineer = Worker(\"DB_Engineer\", api_type=api_type_in_use, api_key=llm_api_key, model=model_in_use, profile=DB_Engineer_profile)\n",
    "        worker_code_reasoner = Worker(\"Code_Reasoner\", api_type=api_type_in_use, api_key=llm_api_key, model=model_in_use, profile=Code_Reasoner_profile)\n",
    "        worker_code_generator = Worker(\"Code_Generator\", api_type=api_type_in_use, api_key=llm_api_key, model=model_in_use, profile=Code_Generator_profile)\n",
    "        worker_assistant = Worker(\"Assistant\", api_type=api_type_in_use, api_key=llm_api_key, model=model_in_use, profile=Assistant_profile)\n",
    "\n",
    "        result = worker_overviewer.process_tasks(prompt=issue_body)\n",
    "        \n",
    "        working_base = []\n",
    "        working_base.append(result[0])\n",
    "\n",
    "        sub_problems = cleanJSON(result[1])\n",
    "        \n",
    "        for sp in sub_problems['sub_problems']:\n",
    "            print(\"## \" + str(sp['title']))\n",
    "            message=\"**TASK**: \" + str(sp['title']) + \"\\n\"\n",
    "            message+=\"inputs: \" + str(sp['inputs']) + \"\\n\"\n",
    "            message+=\"outputs: \" + str(sp['outputs']) + \"\\n\"\n",
    "            message+=\"description: \" + str(sp['description']) + \"\\n\"\n",
    "            \n",
    "            result_decision = worker_decider.process_tasks(prompt=message)\n",
    "            decision_json = cleanJSON(result_decision[0])\n",
    "\n",
    "            print(\"# \" + str(decision_json['decision']))\n",
    "            \n",
    "            if decision_json['decision'] == \"Code Reasoner\":\n",
    "                worker_code_reasoner.add_prompts(working_base)\n",
    "                rcr = worker_code_reasoner.process_tasks(prompt=message)\n",
    "                worker_code_generator.add_prompts(working_base)\n",
    "                worker_code_generator.add_prompt(message)\n",
    "                worker_code_generator.add_prompts(rcr)\n",
    "                result = worker_code_generator.process_tasks()\n",
    "            elif decision_json['decision'] == \"Database Engineer\":\n",
    "                worker_db_engineer.add_prompts(working_base)\n",
    "                worker_db_engineer.add_prompt(message)\n",
    "                result = worker_db_engineer.process_tasks()\n",
    "            else:\n",
    "                worker_assistant.add_prompts(working_base)\n",
    "                worker_assistant.add_prompt(message)\n",
    "                result = worker_assistant.process_tasks()\n",
    "            \n",
    "            print(result)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "        print(exc_type, fname, exc_tb.tb_lineno)\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv[1:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
