{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bee2e26fc1abbdb",
   "metadata": {},
   "source": [
    "# Embeddings and tools tests and conclusions\n",
    "\n",
    "The purpose of this document is to test different approaches in the use of embeddings and tools to improve the context and objectives of the LLM project in terms of code structure and existing functionalities, in order to enhance the development of new code and functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e987376b26a8164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "chroma_db = Chroma(\n",
    "    collection_name=\"codebase\",\n",
    "    embedding_function=embeddings_model,\n",
    "    persist_directory=\"./chroma_db\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec7bbd6f95e74b1",
   "metadata": {},
   "source": [
    "## Pure embeddings\n",
    "\n",
    "Although the use of simple embeddings can improve the LLM's context and give it \"awareness\" of certain existing code snippets that match the request, on their own, they do not provide significant value or context to the LLM, being useful only for identifying repeated or identical functionality in the code.\n",
    "\n",
    "In cases where it is necessary to add a new feature to an existing code snippet (such as a new method in a class), it has been observed that relying solely on embeddings as a code source results in the LLM having great difficulty integrating the feature while keeping the rest of the code in the file intact, as it often needs to be entirely rewritten. This leads to incorrect code and/or the loss of other existing functionalities.\n",
    "\n",
    "Two approaches were tested here:\n",
    "\n",
    "   * Creating embeddings for each existing function in the code.\n",
    "   * Creating embeddings by splitting the document into parts (e.g., 100 characters each).\n",
    " \n",
    "\n",
    "**NOTE**:\n",
    "\n",
    "For better results, the embeddings should be cleaned, going through a process of filtering and removing unnecessary elements, such as extraneous comments or commented-out code. A solution to explore further is the removal of unused functions or unnecessary imports, if they exist. However, this kind of approach requires static code analysis tools, or other tools, that can identify these types of issues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa61238e826ec1a6",
   "metadata": {},
   "source": [
    "### Creating embeddings for each existing function in code\n",
    "\n",
    "To create embeddings for each function, we first need to go through each Python file and extract every function along with the code that makes it up. \n",
    "\n",
    "The following class is used to parse Python code and extract various aspects of it, such as imports, classes, functions, etc., into a JSON structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8324a237295ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "class PythonFileParser(ast.NodeVisitor):\n",
    "    \"\"\"Parse a Python file and return its structure.\"\"\"\n",
    "\n",
    "    def __init__(self, file_name):\n",
    "        self.file_name = file_name\n",
    "        self.imports = []\n",
    "        self.classes = []\n",
    "        self.functions = []\n",
    "        self.constants = []\n",
    "        self.main_block = False\n",
    "        self.global_statements = []\n",
    "        self.comments = []\n",
    "\n",
    "    def _create_func_dict(self, func_node):\n",
    "        # Pre-process the function returns\n",
    "        if not func_node.returns:\n",
    "            returns = None\n",
    "\n",
    "        elif isinstance(func_node.returns, ast.Constant):\n",
    "            returns = func_node.returns.value\n",
    "\n",
    "        elif isinstance(func_node.returns, ast.BinOp):\n",
    "            returns = [func_node.returns.left.id, func_node.returns.right.id]\n",
    "\n",
    "        else:\n",
    "            returns = func_node.returns.id\n",
    "\n",
    "        return {\n",
    "            \"name\": func_node.name,\n",
    "            \"start_line\": func_node.lineno - 1,\n",
    "            \"end_line\": func_node.end_lineno,\n",
    "            \"parameters\": [arg.arg for arg in func_node.args.args],\n",
    "            \"returns\": returns,\n",
    "        }\n",
    "\n",
    "    def visit_Import(self, node):\n",
    "        self.imports.append(\n",
    "            {\n",
    "                \"module\": node.names[0].name,\n",
    "                \"alias\": None,\n",
    "                \"start_line\": node.lineno - 1,\n",
    "                \"end_line\": node.end_lineno,\n",
    "            }\n",
    "        )\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def visit_ImportFrom(self, node):\n",
    "        self.imports.append(\n",
    "            {\n",
    "                \"module\": node.module,\n",
    "                \"alias\": node.names[0].name,\n",
    "                \"start_line\": node.lineno - 1,\n",
    "                \"end_line\": node.end_lineno,\n",
    "            }\n",
    "        )\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def visit_ClassDef(self, node):\n",
    "        tmp_class = {\n",
    "            \"name\": node.name,\n",
    "            \"start_line\": node.lineno - 1,\n",
    "            \"end_line\": node.end_lineno,\n",
    "            \"methods\": [],\n",
    "        }\n",
    "        for func in node.body:\n",
    "            if isinstance(func, ast.FunctionDef):\n",
    "                method_dict = self._create_func_dict(func)\n",
    "                tmp_class[\"methods\"].append(method_dict)\n",
    "                func.is_method = True\n",
    "        self.classes.append(tmp_class)\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def visit_FunctionDef(self, node):\n",
    "        if not hasattr(node, \"is_method\"):\n",
    "            self.functions.append(self._create_func_dict(node))\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def visit_Assign(self, node):\n",
    "        if isinstance(node.targets[0], ast.Name):\n",
    "            value = self._simplify_value(node.value)\n",
    "            const_dict = {\n",
    "                \"name\": node.targets[0].id,\n",
    "                \"start_line\": node.lineno - 1,\n",
    "                \"end_line\": node.end_lineno,\n",
    "            }\n",
    "            if value is not None:\n",
    "                const_dict[\"value\"] = value\n",
    "            self.constants.append(const_dict)\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def visit_If(self, node):\n",
    "        if (\n",
    "            isinstance(node.test, ast.Compare)\n",
    "            and isinstance(node.test.left, ast.Name)\n",
    "            and node.test.left.id == \"__name__\"\n",
    "            and any(isinstance(op, ast.Eq) for op in node.test.ops)\n",
    "            and any(isinstance(cmp, ast.Str) and cmp.s == \"__main__\" for cmp in node.test.comparators)\n",
    "        ):\n",
    "            self.main_block = True\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def visit_Expr(self, node):\n",
    "        if isinstance(node.value, ast.Str):\n",
    "            self.comments.append(\n",
    "                {\n",
    "                    \"type\": \"docstring\",\n",
    "                    \"content\": node.value.s,\n",
    "                    \"start_line\": node.lineno - 1,\n",
    "                    \"end_line\": node.end_lineno,\n",
    "                }\n",
    "            )\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def visit(self, node):\n",
    "        if isinstance(node, ast.Expr) and isinstance(node.value, ast.Str):\n",
    "            self.comments.append(\n",
    "                {\n",
    "                    \"type\": \"docstring\",\n",
    "                    \"content\": node.value.s,\n",
    "                    \"start_line\": node.lineno - 1,\n",
    "                    \"end_line\": node.end_lineno,\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            super().visit(node)\n",
    "\n",
    "    def visit_Module(self, node):\n",
    "        for n in node.body:\n",
    "            if isinstance(n, ast.Expr) and isinstance(n.value, ast.Str):\n",
    "                self.comments.append(\n",
    "                    {\n",
    "                        \"type\": \"docstring\",\n",
    "                        \"content\": n.value.s,\n",
    "                        \"start_line\": n.lineno - 1,\n",
    "                        \"end_line\": n.end_lineno,\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                self.visit(n)\n",
    "\n",
    "    def visit_Global(self, node):\n",
    "        self.global_statements.append(\n",
    "            {\n",
    "                \"type\": \"global\",\n",
    "                \"identifiers\": node.names,\n",
    "                \"start_line\": node.lineno - 1,\n",
    "                \"end_line\": node.end_lineno,\n",
    "            }\n",
    "        )\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def get_structure(self):\n",
    "        return {\n",
    "            \"file_name\": self.file_name,\n",
    "            \"imports\": self.imports,\n",
    "            \"classes\": self.classes,\n",
    "            \"functions\": self.functions,\n",
    "            \"constants\": self.constants,\n",
    "            \"main_block\": self.main_block,\n",
    "            \"global_statements\": self.global_statements,\n",
    "            \"comments\": self.comments,\n",
    "        }\n",
    "\n",
    "    def _simplify_value(self, value):\n",
    "        if isinstance(value, (ast.Str, ast.Num, ast.Constant)):  # Python 3.8+ uses ast.Constant\n",
    "            return value.value if hasattr(value, \"value\") else value.n\n",
    "        elif isinstance(value, ast.NameConstant):\n",
    "            return value.value\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947814fd4e2a422b",
   "metadata": {},
   "source": "Which can be invoked using the parse_python_file function."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296ba8adc2da03d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def parse_python_file(file_path: str) -> str | dict:\n",
    "    \"\"\"Parses a Python file and returns its structure.\"\"\"\n",
    "    if os.path.isdir(file_path):\n",
    "        return \"IS A DIRECTORY\"\n",
    "\n",
    "    with open(file_path, \"r\") as source:\n",
    "        file_content = source.read()\n",
    "    parser = PythonFileParser(file_name=file_path)\n",
    "    tree = ast.parse(file_content, file_path)\n",
    "    parser.visit(tree)\n",
    "    return parser.get_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ee8fbf6232ef94",
   "metadata": {},
   "source": [
    "##### parse_python_file call result\n",
    "\n",
    "Below is the (simplified) result of executing the parse_python_file function on a Python file with imports, classes, and other functions.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"file_name\": \"./utilities.py\",\n",
    "  \"imports\": [\n",
    "    {\n",
    "      \"module\": \"ast\",\n",
    "      \"alias\": null,\n",
    "      \"start_line\": 1,\n",
    "      \"end_line\": 1\n",
    "    },\n",
    "    {\n",
    "      \"module\": \"cProfile\",\n",
    "      \"alias\": null,\n",
    "      \"start_line\": 2,\n",
    "      \"end_line\": 2\n",
    "    }\n",
    "  ],\n",
    "  \"classes\": [\n",
    "    {\n",
    "      \"name\": \"PythonFileParser\",\n",
    "      \"start_line\": 57,\n",
    "      \"end_line\": 230,\n",
    "      \"methods\": [\n",
    "        {\n",
    "          \"name\": \"__init__\",\n",
    "          \"start_line\": 60,\n",
    "          \"end_line\": 68,\n",
    "          \"parameters\": [\"self\", \"file_name\"],\n",
    "          \"returns\": null\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"_create_func_dict\",\n",
    "          \"start_line\": 70,\n",
    "          \"end_line\": 90,\n",
    "          \"parameters\": [\"self\", \"func_node\"],\n",
    "          \"returns\": null\n",
    "        },\n",
    "        ...\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"functions\": [\n",
    "    {\n",
    "      \"name\": \"get_project_files\",\n",
    "      \"start_line\": 15,\n",
    "      \"end_line\": 29,\n",
    "      \"parameters\": [\"directory_path\"],\n",
    "      \"returns\": \"str\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"get_lines_code\",\n",
    "      \"start_line\": 32,\n",
    "      \"end_line\": 42,\n",
    "      \"parameters\": [\"file_path\", \"start_line\", \"end_line\"],\n",
    "      \"returns\": \"str\"\n",
    "    }\n",
    "  ],\n",
    "  \"constants\": [\n",
    "    {\n",
    "      \"name\": \"result\",\n",
    "      \"start_line\": 17,\n",
    "      \"end_line\": 17,\n",
    "      \"value\": \"\"\n",
    "    },\n",
    "    ...\n",
    "  ],\n",
    "  \"comments\": [\n",
    "    {\n",
    "      \"type\": \"docstring\",\n",
    "      \"content\": \"List all Python (.py) files and directories that may contain Python files.\",\n",
    "      \"start_line\": 16,\n",
    "      \"end_line\": 16\n",
    "    },\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699b559e01532a5b",
   "metadata": {},
   "source": "The following function uses the start and end of the lines of code in the previous structure to retrieve the code from the Python file at the specified lines."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbafbbf0aa01d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lines_code(file_path: str, start_line: int, end_line: int) -> str:\n",
    "    \"\"\"Get specific lines of code from a file.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        return f\"{file_path} FILE NOT FOUND\"\n",
    "\n",
    "    elif os.path.isdir(file_path):\n",
    "        return \"IS A DIRECTORY\"\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    return \"\\n\".join(map(lambda s: s.strip(), lines[start_line:end_line]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5477ba5643c290",
   "metadata": {},
   "source": "To generate the embeddings for each file in the project, we still need a function that allows us to list files in a directory:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e8282f610de2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_project_files(directory_path: str = \".\") -> str:\n",
    "    \"\"\"List all Python (.py) files and directories that may contain Python files.\"\"\"\n",
    "    result = \"\"\n",
    "    try:\n",
    "        for file in os.listdir(directory_path):\n",
    "            full_path = os.path.join(directory_path, file)\n",
    "            if os.path.isdir(full_path):\n",
    "                result += f\"{file}/\\n\"\n",
    "            elif file.endswith(\".py\"):\n",
    "                result += f\"{file}\\n\"\n",
    "    except NotADirectoryError:\n",
    "        result = \"NOT A DIRECTORY\"\n",
    "    except FileNotFoundError:\n",
    "        result = \"FILE NOT FOUND\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf1fa590e37f849",
   "metadata": {},
   "source": "Now that we have the necessary functions, we can create embeddings for each existing function in each Python file:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d809967f543fe316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_embeddings(content: str, metadata: list[dict] = None) -> None:\n",
    "    \"\"\"Store embeddings of content with associated metadata.\"\"\"\n",
    "    embedding_vector = embeddings_model.embed_query(content)\n",
    "\n",
    "    if metadata and not isinstance(metadata, list):\n",
    "        metadata = [metadata]\n",
    "\n",
    "    chroma_db.add_texts([content], embeddings=[embedding_vector], metadatas=metadata)\n",
    "\n",
    "\n",
    "def analyze_and_store_code(file_path: str):\n",
    "    \"\"\"Analyze a Python file and store its embeddings, including functions and class methods.\"\"\"\n",
    "    structure = parse_python_file(file_path)\n",
    "\n",
    "    # Store embeddings for standalone functions\n",
    "    for func in structure[\"functions\"]:\n",
    "        code_snippet = get_lines_code(file_path, func[\"start_line\"], func[\"end_line\"])\n",
    "        metadata = {\"name\": func[\"name\"], \"type\": \"function\", \"file\": file_path}\n",
    "        store_embeddings(code_snippet, metadata)\n",
    "\n",
    "    # Store embeddings for classes and their methods\n",
    "    for cls in structure.get(\"classes\", []):\n",
    "        class_snippet = get_lines_code(file_path, cls[\"start_line\"], cls[\"end_line\"])\n",
    "        class_metadata = {\"name\": cls[\"name\"], \"type\": \"class\", \"file\": file_path}\n",
    "\n",
    "        # Store the class code snippet\n",
    "        store_embeddings(class_snippet, class_metadata)\n",
    "\n",
    "        # Store methods within the class\n",
    "        for method in cls.get(\"methods\", []):\n",
    "            method_snippet = get_lines_code(file_path, method[\"start_line\"], method[\"end_line\"])\n",
    "            method_metadata = {\n",
    "                \"name\": method[\"name\"],\n",
    "                \"type\": \"method\",\n",
    "                \"class\": cls[\"name\"],\n",
    "                \"file\": file_path,\n",
    "            }\n",
    "            store_embeddings(method_snippet, method_metadata)\n",
    "\n",
    "\n",
    "# Preprocess: Store embeddings for all project files\n",
    "for file in get_project_files().strip().split(\"\\n\"):\n",
    "    analyze_and_store_code(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7a22db4b1ad41f",
   "metadata": {},
   "source": [
    "#### Results:\n",
    "\n",
    "Although the results are useful for understanding repeated functions or those with similar functionality, this method needs to be further developed. Moreover, when used alone, it does not address the issues raised by the exclusive use of embeddings. Two proposed strategies to enhance the value of the embeddings are:\n",
    "\n",
    "* Adding docstrings and typing for greater context for the LLM and improved results in the embeddings.\n",
    "* Including metadata such as the file and line numbers where it exists, a brief explanation of the code, objectives, and examples of output.\n",
    "\n",
    "It is believed that adding this data to the embeddings improves the results since the LLM does not always search for embeddings by the specific name of the function or by code snippets, but also by expressions or phrases.\n",
    "Although this may improve the results of the embeddings and their usefulness, they need to be complemented with other 'tools' that the LLM can use when it deems appropriate.\n",
    "\n",
    "Note that both docstrings and metadata can (or should) be generated by the LLM itself during an initial process of interpreting and exploring the project.\n",
    "**Another issue that arises from this method is removing old embeddings and updating them with the most recent ones as the project's code is developed or altered.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f3c7c4c1efe97d",
   "metadata": {},
   "source": [
    "### Creating embeddings by splitting the document into parts\n",
    "\n",
    "This other tested method is quite similar to the previous one; however, there is no interpretation of the Python code now. Instead, the file is split into parts of a specified size, and the respective embeddings are calculated and stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a957c0fe01ab85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(python_file_path):\n",
    "    with open(python_file_path, \"r\") as python_file:\n",
    "        source = python_file.read()\n",
    "\n",
    "    # Split documents text\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=100,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents([Document(source, metadata={\"source\": python_file_path})])\n",
    "\n",
    "    # Save to chroma database\n",
    "    chroma_db.from_documents(chunks, embeddings_model, persist_directory=\"./chroma_db\")\n",
    "\n",
    "    print(f\"{len(chunks)} chunks persisted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9ccea725c4d8bf",
   "metadata": {},
   "source": "Now that we have the necessary functions, we can create embeddings for each existing Python file:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc96986794ccff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in get_project_files().strip().split(\"\\n\"):\n",
    "    load_file(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d745df763b5b7e",
   "metadata": {},
   "source": [
    "#### Results:\n",
    "\n",
    "Although this method seems to solve the problem of losing code when asking the LLM to add a new functionality, forcing it to rewrite a file, it is not an absolute method because:\n",
    "\n",
    "* It depends on the size of the embedding splits, which can accommodate an entire file in a single embedding or in parts.\n",
    "* It depends on the dimension of the code file, where the request for embeddings may not result in the complete code from the embeddings, thus maintaining the problem of code loss.\n",
    "* It depends on the terms used by the LLM to search within the embeddings, where results for the file being altered may not even appear.\n",
    "* Large embeddings, and depending on the number of results to retrieve, can easily exceed the token limit that the LLM model accepts as input in a prompt.\n",
    "\n",
    "Additionally, there is the difficulty of improving the context of the embeddings by adding docstrings or code descriptions, as the file is split into pieces, resulting in a loss of flow and logic in the code snippets.\n",
    "\n",
    "Im my results analysis, to maximize the effectiveness of the embeddings, methods that preserve the context and logic of the code should be prioritized, avoiding excessive fragmentation. Therefore,\n",
    "**this should be a method to avoid if we want to make the most out of the embeddings.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706187b03a9649b7",
   "metadata": {},
   "source": [
    "## Pure tools\n",
    "\n",
    "\n",
    "In line with the previous approaches and knowing that some LLMs can use 'tools,' I decided to provide the LLM with some functions that I developed and found relevant. 'Tools' are code functions that are provided to the LLM, which can invoke them if it deems appropriate. They function like a normal Python function, with the exception that the LLM decides when to invoke them and also which arguments to use.\n",
    "\n",
    "\n",
    "### Providing Tools to the LLM\n",
    "\n",
    "\n",
    "The tools should be provided to the LLM following the structure presented here. The function schema (in code bellow) creates this structure. It's important to note that the tools must include typing for both input parameters and return values, and they must also have a docstring that concisely describes what the function does (this helps the LLM understand when to use it). The function's return value must always be of type string, and if it is not, it must be converted to that type.\n",
    "\n",
    "> **SIDE NOTE:**\n",
    "\n",
    "> Not all LLMs support 'tools,' but some tests I conducted demonstrate that if we include something like in the system message or in the context of the prompt:\n",
    "\n",
    "> 'If you need to execute functions, use EXEC \\<function name\\> \\<arguments in JSON\\>'\n",
    "\n",
    "> and then write code to handle these cases, we can achieve almost the same results. Of course, we need to inform the LLM about which functions we have, their purpose, and input arguments, similar to what is done with 'tools' schemas.\n",
    "\n",
    "Following the reasoning of 'What do I, as a human, need to access and obtain code and content from a project,' I arrived at three basic needs:\n",
    "\n",
    "* View the content of a project (list directories)\n",
    "* See what imports, classes, and functions (and other code snippets) a file contains (code structure)\n",
    "* Read parts or the entire code file\n",
    "\n",
    "Based on this, I used the tools I had already developed for the embeddings. Now, it remains to allow the LLM to use them in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa2884ca90b7c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import json\n",
    "import sys\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from openai import OpenAI\n",
    "from pydantic import create_model\n",
    "\n",
    "SYSTEM_MESSAGE = \"\"\"\n",
    "You are an expert Python programmer. You are tasked with enhancing an existing codebase by adding features, \n",
    "writing tests, optimizing performance, fixing bugs, or other typical software development tasks. \n",
    "You have access to specific tools to gather information about the codebase, such as fetching file \n",
    "contents, analyzing file structure, running tests, checking syntax, profiling performance, and more. Use these tools \n",
    "to gather the necessary context and data before making any modifications to the codebase. Do not make assumptions \n",
    "beyond the provided tools and retrieved information.\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Complete the request. You can use the provided tools to get more context of project and files code to complete the request.\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def schema(f):\n",
    "    kw = {\n",
    "        n: (o.annotation, ... if o.default == inspect.Parameter.empty else o.default)\n",
    "        for n, o in inspect.signature(f).parameters.items()\n",
    "    }\n",
    "    s = create_model(f\"Input for {f.__name__}\", **kw).schema()\n",
    "    return {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": dict(name=f.__name__, description=f.__doc__, parameters=s),\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": SYSTEM_MESSAGE}]\n",
    "\n",
    "    llm_tools = [schema(get_lines_code), schema(get_project_files), schema(parse_python_file)]\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"? \")\n",
    "        if not user_input:\n",
    "            continue\n",
    "\n",
    "        elif user_input == \"exit\":\n",
    "            break\n",
    "\n",
    "        prompt = prompt_template.format(question=user_input)\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "        # Request OpenAI\n",
    "        try:\n",
    "            while True:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o\",\n",
    "                    messages=messages,\n",
    "                    tools=llm_tools,\n",
    "                )\n",
    "\n",
    "                messages.append(response.choices[0].message)\n",
    "\n",
    "                if hasattr(response.choices[0].message, \"tool_calls\") and getattr(\n",
    "                    response.choices[0].message, \"tool_calls\"\n",
    "                ):\n",
    "                    # Get the function arguments from model response and call it\n",
    "                    for tool_call in response.choices[0].message.tool_calls:\n",
    "                        function_name = tool_call.function.name\n",
    "                        arguments = json.loads(tool_call.function.arguments)\n",
    "\n",
    "                        print(\"USING TOOLS:\")\n",
    "                        print(\"Using:\", tool_call.function.name, arguments)\n",
    "\n",
    "                        # Call the function with specified arguments\n",
    "                        result = globals()[function_name](**arguments)\n",
    "\n",
    "                        # Ensure result is converted to a string\n",
    "                        str_context_results = json.dumps(result) if isinstance(result, (dict, list)) else str(result)\n",
    "\n",
    "                        print(\"Results:\")\n",
    "                        print(str_context_results)\n",
    "                        print(\"--------------------------------------------------\\n\\n\")\n",
    "\n",
    "                        # Provide de results to the model again\n",
    "                        messages.append(\n",
    "                            {\n",
    "                                \"role\": \"tool\",\n",
    "                                \"name\": tool_call.function.name,\n",
    "                                \"content\": str_context_results,\n",
    "                                \"tool_call_id\": tool_call.id,\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                else:\n",
    "                    print(response.choices[0].message.content)\n",
    "                    break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\", sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5630092d1e5b849c",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "markdown",
   "id": "be1a2dd49ed95a8f",
   "metadata": {},
   "source": [
    "This approach produced significantly better results than the previous ones that relied on embeddings, particularly in solving the problem of adding a new feature without losing parts of the code, in the file, that the embeddings failed to return.\n",
    "\n",
    "With the use of these three tools, when asking the LLM to add a specific functionality to some python file in the project, the approach it generally takes is:\n",
    "\n",
    "* List the files in the project's root directory using get_project_files\n",
    "* Retrieve information about the Python code structure using parse_python_file\n",
    "* Get the entire content of the file from line 0 to the last line using get_lines_of_code\n",
    "* Implement the requested functionality\n",
    "* Return the entire content of the file with the new functionality implemented\n",
    "\n",
    "As observed throughout several experiments, the procedure is always approximately the same, highlighting the need to add a function to read the complete content of a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdc8fbac4331e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_content(file_path: str) -> str:\n",
    "    \"\"\"Get the full content of a file.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        return f\"{file_path} FILE NOT FOUND\"\n",
    "\n",
    "    elif os.path.isdir(file_path):\n",
    "        return \"IS A DIRECTORY\"\n",
    "\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f27fffbc49acf7",
   "metadata": {},
   "source": [
    "It should be noted that the use of these functions raises the same problem previously mentioned for embeddings. The LLM, when reading the complete content of a file, can quickly exhaust the tokens it accepts as input. Therefore, we will need to test an approach to instruct or limit the model's reading of large files at once, as even removing the get_file_content tool, it uses get_lines_of_code to achieve the same end, as mentioned earlier.\n",
    "\n",
    "Another issue that arises is asking the LLM to add a functionality without specifying which file it should do so in. Although in the tested project there is a file called utilities.py, and most of the time new functionalities have been added there correctly, it is uncertain how the model will behave in larger projects. This behavior should be tested in those projects, assessing what action will be taken with and without the context of the project's code.\n",
    "\n",
    "It is likely that new functionalities may end up in unexpected places or may not make sense in the context of the project if the model is not properly instructed on where they should be placed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad4413e306db9a2",
   "metadata": {},
   "source": [
    "#### Providing a function to retrieve results from the embeddings\n",
    "\n",
    "One idea that occurred was, instead of providing the embeddings in the LLM's prompt, to give it a function to search for embeddings when deemed appropriate. Thus, the following function was provided as a tool to the LLM, accepting search terms and a value for k (how many matches to retrieve):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecc52bb23e65e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_embeddings_for_llm(query_text: str, k: int = 5, return_metadata: bool = False) -> list:\n",
    "    \"\"\"Fetch similar content from embeddings for LLM use.\"\"\"\n",
    "    results = chroma_db.search(query=query_text, search_type=\"similarity\", k=k)\n",
    "    if return_metadata:\n",
    "        return [(res.metadata, res.page_content) for res in results]\n",
    "    return [res.page_content for res in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d61b32e751e453",
   "metadata": {},
   "source": [
    "It happens that from the various tests conducted, the LLM always preferred to use the other tools instead of this one. When questioned about this preference, it responded that, lacking prior knowledge of what is contained in the embeddings, it prefers to use other functions over this one.\n",
    "\n",
    "After this response, the system message was changed to indicate that all the code is preloaded and available in the embeddings. Still, after this indication, the model exhibited the same behavior, completely ignoring this component.\n",
    "\n",
    "The code below presents the new system prompt, along with other functions that were developed to create and retrieve embeddings, as well as how this functionality was made available to the LLM. The rest of the code remains unchanged compared to what has been presented so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273723ed25bc150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"\"\"\n",
    "You are an expert Python programmer. You are tasked with enhancing an existing codebase by adding features, \n",
    "writing tests, optimizing performance, fixing bugs, or other typical software development tasks.\n",
    " \n",
    "All project code has been analyzed and stored in embeddings to facilitate more insightful and context-aware interactions. \n",
    "\n",
    "You have access to specific tools to gather information about the codebase, such as fetching file \n",
    "contents, analyzing file structure, and more. Use these tools \n",
    "to gather the necessary context and data before making any modifications to the codebase. Do not make assumptions \n",
    "beyond the provided tools and retrieved information.\n",
    "\"\"\"\n",
    "\n",
    "# ...\n",
    "\n",
    "\n",
    "def store_embeddings(content: str, metadata: list[dict] = None) -> None:  # noqa F811\n",
    "    \"\"\"Store embeddings of content with associated metadata.\"\"\"\n",
    "    embedding_vector = embeddings_model.embed_query(content)\n",
    "\n",
    "    if metadata and not isinstance(metadata, list):\n",
    "        metadata = [metadata]\n",
    "\n",
    "    chroma_db.add_texts([content], embeddings=[embedding_vector], metadatas=metadata)\n",
    "\n",
    "\n",
    "def analyze_and_store_code(file_path: str):\n",
    "    \"\"\"Analyze a Python file and store its embeddings, including functions and class methods.\"\"\"\n",
    "    structure = parse_python_file(file_path)\n",
    "\n",
    "    # Store embeddings for standalone functions\n",
    "    for func in structure[\"functions\"]:\n",
    "        code_snippet = get_lines_code(file_path, func[\"start_line\"], func[\"end_line\"])\n",
    "        metadata = {\"name\": func[\"name\"], \"type\": \"function\", \"file\": file_path}\n",
    "        store_embeddings(code_snippet, metadata)\n",
    "\n",
    "    # Store embeddings for classes and their methods\n",
    "    for cls in structure.get(\"classes\", []):\n",
    "        class_snippet = get_lines_code(file_path, cls[\"start_line\"], cls[\"end_line\"])\n",
    "        class_metadata = {\"name\": cls[\"name\"], \"type\": \"class\", \"file\": file_path}\n",
    "\n",
    "        # Store the class code snippet\n",
    "        store_embeddings(class_snippet, class_metadata)\n",
    "\n",
    "        # Store methods within the class\n",
    "        for method in cls.get(\"methods\", []):\n",
    "            method_snippet = get_lines_code(file_path, method[\"start_line\"], method[\"end_line\"])\n",
    "            method_metadata = {\n",
    "                \"name\": method[\"name\"],\n",
    "                \"type\": \"method\",\n",
    "                \"class\": cls[\"name\"],\n",
    "                \"file\": file_path,\n",
    "            }\n",
    "            store_embeddings(method_snippet, method_metadata)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Preprocess: Store embeddings for all project files\n",
    "    for file in get_project_files().strip().split(\"\\n\"):\n",
    "        analyze_and_store_code(file)\n",
    "\n",
    "    client = OpenAI()\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": SYSTEM_MESSAGE}]\n",
    "\n",
    "    llm_tools = [\n",
    "        schema(get_lines_code),\n",
    "        schema(get_project_files),\n",
    "        schema(parse_python_file),\n",
    "        schema(store_embeddings),\n",
    "        schema(query_embeddings_for_llm),\n",
    "    ]\n",
    "\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7f1d25f1f6fff9",
   "metadata": {},
   "source": [
    "It is worth noting that we also chose to provide a function for storing embeddings (store_embeddings), which also proved to be of no utility to the LLM.\n",
    "\n",
    "Although the LLM has consistently opted not to use embeddings through query_embeddings_for_llm, it may be necessary to improve the system message and/or prompt to place more emphasis on this functionality. However, this option still requires testing, as it has not been asked, for example, for the LLM to determine whether a specific function already provides a certain functionality or if there is similar code scattered throughout the project. Other similar and relevant questions should be posed, in terms of code refactoring rather than development, to analyze what utility the LLM derives from this type of function, which is more suited to the cases now presented.\n",
    "\n",
    " Based on the behavior observed so far, it is believed that the LLM's choice will continue to lean towards avoiding embeddings in favor of more direct functions, even if they require more operations and time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f7cd8067552e3f",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The use of the three initial tools, followed by the addition of another to read complete files, were undoubtedly the most preferred and utilized by the LLM during the conducted tests. They can also be crucial, when combined with other functionalities, to provide a greater global context of the project even before being assigned to the LLM, as was the case when using them to generate embeddings from the existing code.\n",
    "\n",
    "Regarding the tools for storing and retrieving embeddings, they did not prove to be as useful from the LLM's perspective, likely due to their unpredictability compared to the others, where the results are more concise, accurate, and deterministic than this other method.\n",
    "\n",
    "It is necessary to conduct more tests on the system message and the prompt, and to analyze how the model reacts to these changes. Additionally, the same tests should be performed with other models to understand which one or which are suitable for the objective at hand, as well as how they respond to the same prompts and system messages.\n",
    "\n",
    "After these tests, it was decided to combine the two approaches, that is: to provide the results of the embeddings in the prompt while simultaneously offering the tools created and already described to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27f860d5ed1459d",
   "metadata": {},
   "source": [
    "## Combine embeddings and tools\n",
    "\n",
    "Now, let's explain the process followed to combine the previous methods, that is, to inject the results of the embeddings into the prompt while also providing the tools to the LLM. It is worth noting in advance that this is a case that still requires extensive testing, as this approach has been evaluated lightly.\n",
    "\n",
    "Here, all the tools previously made available were used. The main difference was that the results of the embeddings were incorporated into the prompt introduced into the LLM.\n",
    "\n",
    "The method of embeddings used was the creation of structured embeddings according to the code, and the approach of splitting files into chunks and generating embeddings from them was not tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36433fe0be04fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"? \")\n",
    "    if not user_input:\n",
    "        continue\n",
    "    elif user_input == \"exit\":\n",
    "        break\n",
    "\n",
    "    # Retrieve similar code snippets using embeddings\n",
    "    similar_content = query_embeddings_for_llm(user_input)\n",
    "\n",
    "    # Construct the AI prompt with embedding-derived context\n",
    "    prompt = prompt_template.format(request=user_input, similar_code=\"\\n\".join(similar_content))\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4957762222e166f5",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Although this method has not been exhaustively tested, the few tests conducted suggest that the LLM tends to prefer the tools, seemingly ignoring the information provided in the context altogether.\n",
    "\n",
    "One option to explore here is to say **\"using the examples that follow\"** instead of **\"using the context that follows\"**. Perhaps this slight change could trigger a different behavior from the LLM and encourage it to pay more attention to what is provided by the embeddings.\n",
    "\n",
    "From this approach, another idea emerged, which, in summary, consists of maintaining a database of code embeddings used to solve specific problems in other projects that can be provided as examples for similar functionalities.\n",
    "\n",
    "It should be mentioned again that this approach has been very minimally tested and may yield relevant results. Additionally, it is advisable to test changing the prompt or the system message to see their influence on the LLM and whether it pays more attention to the context provided to it. Furthermore, it should be tested in larger projects to verify whether the LLM truly benefits from the embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83cb7fa4366af06",
   "metadata": {},
   "source": [
    "## Embeddings across different projects\n",
    "\n",
    "\n",
    "This approach has not been tested, and it is believed that human intervention may be necessary. However, it could be worth exploring the use of a persistent embeddings database where curated solutions that are similar across various projects and that proven to work can be stored. For example, the use of rest_framework, where there are sometimes custom methods developed by us programmers that solve common problems in an API.\n",
    "\n",
    "It is essential to test the feasibility of what has been mentioned, to ascertain the actual benefit for the LLM, and to determine the human effort required to identify which solutions deserve to be stored. This does not take into account the need to add extra information to these solutions for the LLM to truly leverage the embeddings. However, it is likely that this last part can be done by the LLM itself before generating the embeddings.\n",
    "\n",
    "It is also important to define human criteria to help decide what should be stored. This may include the frequency with which the solution is used, its complexity, or its utility in different contexts and projects.\n",
    "\n",
    "The metadata accompanying these solutions should be studied. For example:\n",
    "\n",
    "* Context of the problem and applicability.\n",
    "* Specific problem it solves\n",
    "* Usage examples\n",
    "* Possible limitations\n",
    "\n",
    "Although it has been mentioned that the LLM can perform this analysis, it remains relevant to have a human perspective to verify the depth of the problem or nuances in the solution that the LLM may not have captured.\n",
    "\n",
    "I asked ChatGPT about this approach to get other opinion. I leave her response here:\n",
    "\n",
    "> The proposed approach presents a promising strategy to maximize the use of embeddings, especially in contexts where solutions are frequently reused. The combination of persistence, human curation, and rigorous testing can lead to significant results. However, it is vital to implement a clear and structured process to ensure that the stored solutions are truly useful and that the LLM can access them effectively.\n",
    "Suggestions for Further Exploration\n",
    "\n",
    "> * Case Studies: Conduct case studies on various projects to see how different approaches work in practice.\n",
    "> * LLM Feedback: Consider implementing a mechanism for the LLM to provide feedback on the usefulness of the stored solutions, assisting in the ongoing curation of the database.\n",
    "> * Rapid Iteration: Adopt a rapid iteration cycle in testing and implementations, allowing for adjustments as new information and results become available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610e76229afa99ba",
   "metadata": {},
   "source": [
    "## Conversation history\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230ebbd25eac52bf",
   "metadata": {},
   "source": [
    "## Prompt enginneering\n",
    "\n",
    "[Prompting guide](https://www.promptingguide.ai/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
