{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Embeddings and \"tools\" test\n",
    "\n",
    "The purpose of this document is to test different approaches in the use of embeddings and tools to improve the context and objectives of the LLM project in terms of code structure and existing functionalities, in order to enhance the development of new code and functionalities."
   ],
   "id": "1bee2e26fc1abbdb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "chroma_db = Chroma(\n",
    "    collection_name=\"codebase\",\n",
    "    embedding_function=embeddings_model,\n",
    "    persist_directory=\"./chroma_db\",\n",
    ")"
   ],
   "id": "e987376b26a8164a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pure embeddings\n",
    "\n",
    "Although the use of simple embeddings can improve the LLM's context and give it \"awareness\" of certain existing code snippets that match the request, on their own, they do not provide significant value or context to the LLM, being useful only for identifying repeated or identical functionality in the code.\n",
    "\n",
    "In cases where it is necessary to add a new feature to an existing code snippet (such as a new method in a class), it has been observed that relying solely on embeddings as a code source results in the LLM having great difficulty integrating the feature while keeping the rest of the code in the file intact, as it often needs to be entirely rewritten. This leads to incorrect code and/or the loss of other existing functionalities.\n",
    "\n",
    "Two approaches were tested here:\n",
    "\n",
    "   * Creating embeddings for each existing function in the code.\n",
    "   * Creating embeddings by splitting the document into parts (e.g., 100 characters each).\n"
   ],
   "id": "aec7bbd6f95e74b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Creating embeddings for each existing function in code\n",
    "\n",
    "To create embeddings for each function, we first need to go through each Python file and extract every function along with the code that makes it up. \n",
    "\n",
    "The following class is used to parse Python code and extract various aspects of it, such as imports, classes, functions, etc., into a JSON structure:"
   ],
   "id": "aa61238e826ec1a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import ast\n",
    "\n",
    "class PythonFileParser(ast.NodeVisitor):\n",
    "    \"\"\"Parse a Python file and return its structure.\"\"\"\n",
    "\n",
    "    def __init__(self, file_name):\n",
    "        self.file_name = file_name\n",
    "        self.imports = []\n",
    "        self.classes = []\n",
    "        self.functions = []\n",
    "        self.constants = []\n",
    "        self.main_block = False\n",
    "        self.global_statements = []\n",
    "        self.comments = []\n",
    "\n",
    "    def _create_func_dict(self, func_node):\n",
    "        # Pre-process the function returns\n",
    "        if not func_node.returns:\n",
    "            returns = None\n",
    "\n",
    "        elif isinstance(func_node.returns, ast.Constant):\n",
    "            returns = func_node.returns.value\n",
    "\n",
    "        elif isinstance(func_node.returns, ast.BinOp):\n",
    "            returns = [func_node.returns.left.id, func_node.returns.right.id]\n",
    "\n",
    "        else:\n",
    "            returns = func_node.returns.id\n",
    "\n",
    "        return {\n",
    "            \"name\": func_node.name,\n",
    "            \"start_line\": func_node.lineno,\n",
    "            \"end_line\": func_node.end_lineno,\n",
    "            \"parameters\": [arg.arg for arg in func_node.args.args],\n",
    "            \"returns\": returns,\n",
    "        }\n",
    "\n",
    "    def visit_Import(self, node):\n",
    "        self.imports.append(\n",
    "            {\n",
    "                \"module\": node.names[0].name,\n",
    "                \"alias\": None,\n",
    "                \"start_line\": node.lineno,\n",
    "                \"end_line\": node.end_lineno,\n",
    "            }\n",
    "        )\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def visit_ImportFrom(self, node):\n",
    "        self.imports.append(\n",
    "            {\n",
    "                \"module\": node.module,\n",
    "                \"alias\": node.names[0].name,\n",
    "                \"start_line\": node.lineno,\n",
    "                \"end_line\": node.end_lineno,\n",
    "            }\n",
    "        )\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def visit_ClassDef(self, node):\n",
    "        tmp_class = {\n",
    "            \"name\": node.name,\n",
    "            \"start_line\": node.lineno,\n",
    "            \"end_line\": node.end_lineno,\n",
    "            \"methods\": [],\n",
    "        }\n",
    "        for func in node.body:\n",
    "            if isinstance(func, ast.FunctionDef):\n",
    "                method_dict = self._create_func_dict(func)\n",
    "                tmp_class[\"methods\"].append(method_dict)\n",
    "                func.is_method = True\n",
    "        self.classes.append(tmp_class)\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def visit_FunctionDef(self, node):\n",
    "        if not hasattr(node, \"is_method\"):\n",
    "            self.functions.append(self._create_func_dict(node))\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def visit_Assign(self, node):\n",
    "        if isinstance(node.targets[0], ast.Name):\n",
    "            value = self._simplify_value(node.value)\n",
    "            const_dict = {\n",
    "                \"name\": node.targets[0].id,\n",
    "                \"start_line\": node.lineno,\n",
    "                \"end_line\": node.end_lineno,\n",
    "            }\n",
    "            if value is not None:\n",
    "                const_dict[\"value\"] = value\n",
    "            self.constants.append(const_dict)\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def visit_If(self, node):\n",
    "        if (\n",
    "            isinstance(node.test, ast.Compare)\n",
    "            and isinstance(node.test.left, ast.Name)\n",
    "            and node.test.left.id == \"__name__\"\n",
    "            and any(isinstance(op, ast.Eq) for op in node.test.ops)\n",
    "            and any(\n",
    "                isinstance(cmp, ast.Str) and cmp.s == \"__main__\"\n",
    "                for cmp in node.test.comparators\n",
    "            )\n",
    "        ):\n",
    "            self.main_block = True\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def visit_Expr(self, node):\n",
    "        if isinstance(node.value, ast.Str):\n",
    "            self.comments.append(\n",
    "                {\n",
    "                    \"type\": \"docstring\",\n",
    "                    \"content\": node.value.s,\n",
    "                    \"start_line\": node.lineno,\n",
    "                    \"end_line\": node.end_lineno,\n",
    "                }\n",
    "            )\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def visit(self, node):\n",
    "        if isinstance(node, ast.Expr) and isinstance(node.value, ast.Str):\n",
    "            self.comments.append(\n",
    "                {\n",
    "                    \"type\": \"docstring\",\n",
    "                    \"content\": node.value.s,\n",
    "                    \"start_line\": node.lineno,\n",
    "                    \"end_line\": node.end_lineno,\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            super().visit(node)\n",
    "\n",
    "    def visit_Module(self, node):\n",
    "        for n in node.body:\n",
    "            if isinstance(n, ast.Expr) and isinstance(n.value, ast.Str):\n",
    "                self.comments.append(\n",
    "                    {\n",
    "                        \"type\": \"docstring\",\n",
    "                        \"content\": n.value.s,\n",
    "                        \"start_line\": n.lineno,\n",
    "                        \"end_line\": n.end_lineno,\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                self.visit(n)\n",
    "\n",
    "    def visit_Global(self, node):\n",
    "        self.global_statements.append(\n",
    "            {\n",
    "                \"type\": \"global\",\n",
    "                \"identifiers\": node.names,\n",
    "                \"start_line\": node.lineno,\n",
    "                \"end_line\": node.end_lineno,\n",
    "            }\n",
    "        )\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def get_structure(self):\n",
    "        return {\n",
    "            \"file_name\": self.file_name,\n",
    "            \"imports\": self.imports,\n",
    "            \"classes\": self.classes,\n",
    "            \"functions\": self.functions,\n",
    "            \"constants\": self.constants,\n",
    "            \"main_block\": self.main_block,\n",
    "            \"global_statements\": self.global_statements,\n",
    "            \"comments\": self.comments,\n",
    "        }\n",
    "\n",
    "    def _simplify_value(self, value):\n",
    "        if isinstance(\n",
    "            value, (ast.Str, ast.Num, ast.Constant)\n",
    "        ):  # Python 3.8+ uses ast.Constant\n",
    "            return value.value if hasattr(value, \"value\") else value.n\n",
    "        elif isinstance(value, ast.NameConstant):\n",
    "            return value.value\n",
    "        return None"
   ],
   "id": "5b8324a237295ed1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Which can be invoked using the parse_python_file function.",
   "id": "947814fd4e2a422b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "def parse_python_file(file_path: str) -> str | dict:\n",
    "    \"\"\"Parses a Python file and returns its structure.\"\"\"\n",
    "    if os.path.isdir(file_path):\n",
    "        return \"IS A DIRECTORY\"\n",
    "\n",
    "    with open(file_path, \"r\") as source:\n",
    "        file_content = source.read()\n",
    "    parser = PythonFileParser(file_name=file_path)\n",
    "    tree = ast.parse(file_content, file_path)\n",
    "    parser.visit(tree)\n",
    "    return parser.get_structure()"
   ],
   "id": "296ba8adc2da03d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### parse_python_file call result\n",
    "\n",
    "Below is the (simplified) result of executing the parse_python_file function on a Python file with imports, classes, and other functions.\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"file_name\": \"./utilities.py\",\n",
    "  \"imports\": [\n",
    "    {\n",
    "      \"module\": \"ast\",\n",
    "      \"alias\": null,\n",
    "      \"start_line\": 1,\n",
    "      \"end_line\": 1\n",
    "    },\n",
    "    {\n",
    "      \"module\": \"cProfile\",\n",
    "      \"alias\": null,\n",
    "      \"start_line\": 2,\n",
    "      \"end_line\": 2\n",
    "    }\n",
    "  ],\n",
    "  \"classes\": [\n",
    "    {\n",
    "      \"name\": \"PythonFileParser\",\n",
    "      \"start_line\": 57,\n",
    "      \"end_line\": 230,\n",
    "      \"methods\": [\n",
    "        {\n",
    "          \"name\": \"__init__\",\n",
    "          \"start_line\": 60,\n",
    "          \"end_line\": 68,\n",
    "          \"parameters\": [\"self\", \"file_name\"],\n",
    "          \"returns\": null\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"_create_func_dict\",\n",
    "          \"start_line\": 70,\n",
    "          \"end_line\": 90,\n",
    "          \"parameters\": [\"self\", \"func_node\"],\n",
    "          \"returns\": null\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"functions\": [\n",
    "    {\n",
    "      \"name\": \"get_project_files\",\n",
    "      \"start_line\": 15,\n",
    "      \"end_line\": 29,\n",
    "      \"parameters\": [\"directory_path\"],\n",
    "      \"returns\": \"str\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"get_lines_code\",\n",
    "      \"start_line\": 32,\n",
    "      \"end_line\": 42,\n",
    "      \"parameters\": [\"file_path\", \"start_line\", \"end_line\"],\n",
    "      \"returns\": \"str\"\n",
    "    }\n",
    "  ],\n",
    "  \"constants\": [\n",
    "    {\n",
    "      \"name\": \"result\",\n",
    "      \"start_line\": 17,\n",
    "      \"end_line\": 17,\n",
    "      \"value\": \"\"\n",
    "    }\n",
    "  ],\n",
    "  \"comments\": [\n",
    "    {\n",
    "      \"type\": \"docstring\",\n",
    "      \"content\": \"List all Python (.py) files and directories that may contain Python files.\",\n",
    "      \"start_line\": 16,\n",
    "      \"end_line\": 16\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ],
   "id": "29ee8fbf6232ef94"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The following function uses the start and end of the lines of code in the previous structure to retrieve the code from the Python file at the specified lines.",
   "id": "699b559e01532a5b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_lines_code(file_path: str, start_line: int, end_line: int) -> str:\n",
    "    \"\"\"Get specific lines of code from a file.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        return f\"{file_path} FILE NOT FOUND\"\n",
    "\n",
    "    elif os.path.isdir(file_path):\n",
    "        return \"IS A DIRECTORY\"\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    return \"\\n\".join(map(lambda s: s.strip(), lines[start_line:end_line]))"
   ],
   "id": "9cbafbbf0aa01d46"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To generate the embeddings for each file in the project, we still need a function that allows us to list files in a directory:",
   "id": "bf5477ba5643c290"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_project_files(directory_path: str = \".\") -> str:\n",
    "    \"\"\"List all Python (.py) files and directories that may contain Python files.\"\"\"\n",
    "    result = \"\"\n",
    "    try:\n",
    "        for file in os.listdir(directory_path):\n",
    "            full_path = os.path.join(directory_path, file)\n",
    "            if os.path.isdir(full_path):\n",
    "                result += f\"{file}/\\n\"\n",
    "            elif file.endswith(\".py\"):\n",
    "                result += f\"{file}\\n\"\n",
    "    except NotADirectoryError:\n",
    "        result = \"NOT A DIRECTORY\"\n",
    "    except FileNotFoundError:\n",
    "        result = \"FILE NOT FOUND\"\n",
    "    return result"
   ],
   "id": "66e8282f610de2c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that we have the necessary functions, we can create embeddings for each existing function in each Python file:",
   "id": "edf1fa590e37f849"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def store_embeddings(content: str, metadata: list[dict] = None) -> None:\n",
    "    \"\"\"Store embeddings of content with associated metadata.\"\"\"\n",
    "    embedding_vector = embeddings_model.embed_query(content)\n",
    "\n",
    "    if metadata and not isinstance(metadata, list):\n",
    "        metadata = [metadata]\n",
    "\n",
    "    chroma_db.add_texts([content], embeddings=[embedding_vector], metadatas=metadata)\n",
    "    \n",
    "def analyze_and_store_code(file_path: str):\n",
    "    \"\"\"Analyze a Python file and store its embeddings, including functions and class methods.\"\"\"\n",
    "    structure = parse_python_file(file_path)\n",
    "\n",
    "    # Store embeddings for standalone functions\n",
    "    for func in structure[\"functions\"]:\n",
    "        code_snippet = get_lines_code(\n",
    "            file_path, func[\"start_line\"], func[\"end_line\"]\n",
    "        )\n",
    "        metadata = {\"name\": func[\"name\"], \"type\": \"function\", \"file\": file_path}\n",
    "        store_embeddings(code_snippet, metadata)\n",
    "\n",
    "    # Store embeddings for classes and their methods\n",
    "    for cls in structure.get(\"classes\", []):\n",
    "        class_snippet = get_lines_code(\n",
    "            file_path, cls[\"start_line\"], cls[\"end_line\"]\n",
    "        )\n",
    "        class_metadata = {\"name\": cls[\"name\"], \"type\": \"class\", \"file\": file_path}\n",
    "\n",
    "        # Store the class code snippet\n",
    "        store_embeddings(class_snippet, class_metadata)\n",
    "\n",
    "        # Store methods within the class\n",
    "        for method in cls.get(\"methods\", []):\n",
    "            method_snippet = get_lines_code(\n",
    "                file_path, method[\"start_line\"], method[\"end_line\"]\n",
    "            )\n",
    "            method_metadata = {\n",
    "                \"name\": method[\"name\"],\n",
    "                \"type\": \"method\",\n",
    "                \"class\": cls[\"name\"],\n",
    "                \"file\": file_path,\n",
    "            }\n",
    "            store_embeddings(method_snippet, method_metadata)\n",
    "            \n",
    "# Preprocess: Store embeddings for all project files\n",
    "for file in get_project_files().strip().split(\"\\n\"):\n",
    "    analyze_and_store_code(file)"
   ],
   "id": "d809967f543fe316"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Results:\n",
    "\n",
    "Although the results are useful for understanding repeated functions or those with similar functionality, this method needs to be further developed. Moreover, when used alone, it does not address the issues raised by the exclusive use of embeddings. Two proposed strategies to enhance the value of the embeddings are:\n",
    "\n",
    "* Adding docstrings and typing for greater context for the LLM and improved results in the embeddings.\n",
    "* Including metadata such as the file and line numbers where it exists, a brief explanation of the code, objectives, and examples of output.\n",
    "\n",
    "It is believed that adding this data to the embeddings improves the results since the LLM does not always search for embeddings by the specific name of the function or by code snippets, but also by expressions or phrases.\n",
    "Although this may improve the results of the embeddings and their usefulness, they need to be complemented with other 'tools' that the LLM can use when it deems appropriate.\n",
    "\n",
    "Note that both docstrings and metadata can (or should) be generated by the LLM itself during an initial process of interpreting and exploring the project.\n",
    "Another issue that arises from this method is removing old embeddings and updating them with the most recent ones as the project's code is developed or altered."
   ],
   "id": "4f7a22db4b1ad41f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Creating embeddings by splitting the document into parts\n",
    "\n",
    "This other tested method is quite similar to the previous one; however, there is no interpretation of the Python code now. Instead, the file is split into parts of a specified size, and the respective embeddings are calculated and stored."
   ],
   "id": "98f3c7c4c1efe97d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_file(python_file_path):\n",
    "    with open(python_file_path, \"r\") as python_file:\n",
    "        source = python_file.read()\n",
    "\n",
    "    # Split documents text\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=100,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(\n",
    "        [Document(source, metadata={\"source\": python_file_path})]\n",
    "    )\n",
    "\n",
    "    # Save to chroma database\n",
    "    chroma_db.from_documents(chunks, embeddings_model, persist_directory=\"./chroma_db\")\n",
    "\n",
    "    print(f\"{len(chunks)} chunks persisted\")"
   ],
   "id": "6a957c0fe01ab85a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that we have the necessary functions, we can create embeddings for each existing Python file:",
   "id": "5a9ccea725c4d8bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for file in get_project_files().strip().split(\"\\n\"):\n",
    "    load_file(file)"
   ],
   "id": "4bc96986794ccff5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Results:\n",
    "\n",
    "Although this method seems to solve the problem of losing code when asking the LLM to add a new functionality, forcing it to rewrite a file, it is not an absolute method because:\n",
    "\n",
    "* It depends on the size of the embedding splits, which can accommodate an entire file in a single embedding or in parts.\n",
    "* It depends on the dimension of the code file, where the request for embeddings may not result in the complete code from the embeddings, thus maintaining the problem of code loss.\n",
    "* It depends on the terms used by the LLM to search within the embeddings, where results for the file being altered may not even appear.\n",
    "\n",
    "Additionally, there is the difficulty of improving the context of the embeddings by adding docstrings or code descriptions, as the file is split into pieces, resulting in a loss of flow and logic in the code snippets.\n",
    "\n",
    "**Definitely, this should be a method to avoid if we want to make the most out of the embeddings.**"
   ],
   "id": "55d745df763b5b7e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
